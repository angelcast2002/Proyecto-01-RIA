<h1 id="informe-sobre-mitigacion-de-sesgos-en-modelos-predictivos-adult-census-income-dataset">Informe sobre Mitigación de Sesgos en Modelos Predictivos: Adult Census Income Dataset</h1>
<h2 id="1-introduccion-y-contexto">1. Introducción y Contexto</h2>
<p>La Inteligencia Artificial se ha consolidado como una herramienta capaz de transformar industrias, optimizar procesos y mejorar la vida de las personas. No obstante, el uso de sistemas de IA conlleva un riesgo importante: la reproducción y, en ocasiones, la amplificación de desigualdades sociales preexistentes. Estas desigualdades pueden aparecer porque los datos contienen sesgos de origen histórico, porque los algoritmos están diseñados de manera que favorecen ciertos resultados, o porque la interpretación humana de los modelos carece de un análisis crítico de equidad.</p>
<p>El marco de <strong>IA Responsable</strong>, que se articula en torno a los principios conocidos como FAT o FATE, busca enfrentar estos riesgos. La equidad consiste en evitar que un sistema discrimine injustamente a individuos o grupos en función de atributos sensibles como el género o la raza. La responsabilidad apunta a establecer mecanismos claros para monitorear y corregir errores o sesgos. La transparencia exige que los modelos sean comprensibles y que las decisiones puedan ser auditadas, mientras que la ética procura que la IA se alinee con valores humanos y persiga un beneficio social.</p>
<p>En este trabajo se aplican dichos principios al análisis del conjunto de datos Adult Census Income, disponible en la UCI Machine Learning Repository. El objetivo de este dataset es predecir si una persona percibe un ingreso anual mayor a 50,000 dólares, utilizando información demográfica como edad, género, raza, educación u ocupación. Este conjunto resulta especialmente pertinente porque es conocido por reflejar desigualdades salariales históricas entre hombres y mujeres, así como entre diferentes grupos raciales.</p>
<p>El proyecto persigue varios objetivos: explorar los datos e identificar posibles fuentes de sesgo, entrenar un modelo predictivo de referencia, evaluar si el modelo hereda o amplifica las desigualdades observadas en los datos, aplicar estrategias de mitigación para reducir las disparidades y reflexionar sobre los riesgos éticos de utilizar modelos de este tipo en contextos reales.</p>
<h2 id="2-analisis-exploratorio-de-datos">2. Análisis Exploratorio de Datos</h2>
<p>El análisis exploratorio permitió identificar de manera preliminar las fuentes de sesgo presentes en los datos. La variable objetivo, que corresponde al nivel de ingresos, muestra un fuerte desbalance: más de tres cuartas partes de los individuos (75.9%) ganan menos de 50,000 dólares al año, mientras que apenas el 24.1% supera este umbral. Este desbalance, aunque reflejo de la realidad socioeconómica, implica que el modelo se enfrentará a una tarea de clasificación desequilibrada, lo que puede dificultar la identificación precisa de los casos minoritarios.</p>
<p>En cuanto a los atributos sensibles, se observa que el 66.9% de los registros corresponden a hombres y el 33.1% a mujeres. Desde el punto de vista racial, el 85.4% de los individuos se identifican como blancos, mientras que el 9.6% son negros y el resto se distribuye en proporciones mucho menores entre asiáticos, amerindios y la categoría “otros”. Esto implica que tanto las mujeres como las minorías raciales se encuentran sobrerrepresentadas en las categorías de menor ingreso, lo cual constituye una posible fuente de sesgo.</p>
<p>Al desagregar los ingresos por grupo se confirma esta situación. Entre los hombres, el 30.6% gana más de 50,000 dólares, mientras que entre las mujeres esta proporción cae drásticamente al 10.9%. La disparidad también se evidencia entre grupos raciales: el 25.6% de los blancos superan el umbral de ingresos, frente al 12.4% de los negros, el 11.6% de los amerindios y apenas el 9.2% de quienes se encuentran en la categoría “otros”. Estos resultados confirman que los datos de entrenamiento reflejan desigualdades estructurales y que, de no intervenirse, un modelo predictivo basado en este conjunto reproduciría dichas inequidades en sus predicciones.</p>
<h2 id="3-modelo-baseline">3. Modelo Baseline</h2>
<p>El modelo de referencia consistió en una regresión logística, precedida de un preprocesamiento que incluyó la estandarización de variables numéricas y la codificación <em>One Hot</em> para las categóricas. En el conjunto de prueba, este modelo alcanzó una exactitud del 85.4%, una precisión de 74.2%, un recall de 60.5% y una puntuación F1 de 66.7%. El área bajo la curva ROC fue de 0.90, lo cual indica una buena capacidad de discriminación general.</p>
<p><strong>Tabla 1. Métricas globales del modelo baseline</strong></p>
<table>
<thead>
<tr>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
<th>ROC AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.854</td>
<td>0.742</td>
<td>0.605</td>
<td>0.667</td>
<td>0.900</td>
</tr>
</tbody>
</table>
<p>Sin embargo, las métricas globales ocultan las disparidades entre grupos. Al segmentar por sexo, se encontró que el modelo predice ingresos altos para el 25.2% de los hombres, pero únicamente para el 8.2% de las mujeres. De manera similar, al segmentar por raza, el 20.8% de los blancos son clasificados como ganadores de más de 50,000 dólares, mientras que la proporción cae al 9.9% en negros y a aproximadamente un 11% en otras minorías.</p>
<p><strong>Tabla 2. Métricas por sexo (baseline)</strong></p>
<table>
<thead>
<tr>
<th>Grupo</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
<th>Selection Rate</th>
<th>TPR</th>
<th>FPR</th>
</tr>
</thead>
<tbody>
<tr>
<td>Female</td>
<td>0.929</td>
<td>0.733</td>
<td>0.551</td>
<td>0.629</td>
<td>0.082</td>
<td>0.551</td>
<td>0.025</td>
</tr>
<tr>
<td>Male</td>
<td>0.818</td>
<td>0.744</td>
<td>0.615</td>
<td>0.673</td>
<td>0.252</td>
<td>0.615</td>
<td>0.093</td>
</tr>
</tbody>
</table>
<p><img alt="alt text" src="../image.png" /></p>
<p><strong>Tabla 3. Métricas por raza (baseline)</strong></p>
<table>
<thead>
<tr>
<th>Grupo</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1</th>
<th>Selection Rate</th>
<th>TPR</th>
<th>FPR</th>
</tr>
</thead>
<tbody>
<tr>
<td>Amer-Indian-Eskimo</td>
<td>0.873</td>
<td>0.444</td>
<td>0.444</td>
<td>0.444</td>
<td>0.114</td>
<td>0.444</td>
<td>0.071</td>
</tr>
<tr>
<td>Asian-Pac-Islander</td>
<td>0.825</td>
<td>0.698</td>
<td>0.561</td>
<td>0.622</td>
<td>0.206</td>
<td>0.561</td>
<td>0.084</td>
</tr>
<tr>
<td>Black</td>
<td>0.914</td>
<td>0.681</td>
<td>0.557</td>
<td>0.612</td>
<td>0.099</td>
<td>0.557</td>
<td>0.036</td>
</tr>
<tr>
<td>Other</td>
<td>0.937</td>
<td>0.429</td>
<td>1.000</td>
<td>0.600</td>
<td>0.111</td>
<td>1.000</td>
<td>0.067</td>
</tr>
<tr>
<td>White</td>
<td>0.848</td>
<td>0.750</td>
<td>0.609</td>
<td>0.672</td>
<td>0.208</td>
<td>0.609</td>
<td>0.070</td>
</tr>
</tbody>
</table>
<p><img alt="alt text" src="../image-1.png" /></p>
<p>Las métricas de disparidad confirman estas brechas. En el caso del sexo, la diferencia de paridad demográfica (dp_diff) alcanzó un valor de 0.169, mientras que el ratio de paridad fue de apenas 0.327, lo que significa que las mujeres tienen un tercio de las oportunidades de ser clasificadas con ingresos altos en comparación con los hombres. El indicador de igualdad de oportunidades (eo_diff) fue de 0.068, lo que refleja diferencias significativas en la tasa de verdaderos positivos. En el caso de la raza, los valores fueron igualmente preocupantes: dp_diff de 0.108, dp_ratio de 0.478 y eo_diff de 0.556.</p>
<p>Estos resultados permiten concluir que el modelo baseline no solo refleja las desigualdades de los datos, sino que contribuye a amplificarlas en sus predicciones.</p>
<h2 id="4-estrategias-de-mitigacion">4. Estrategias de Mitigación</h2>
<p>Con el fin de reducir estas disparidades se implementaron tres enfoques distintos. El primero fue la técnica de <strong>Equalized Odds</strong> mediante el algoritmo de <em>Exponentiated Gradient</em>, que busca balancear las tasas de verdaderos positivos y falsos positivos entre grupos sensibles. Con esta técnica, la disparidad en sexo se redujo de un eo_diff de 0.068 a 0.031, mientras que en raza se redujo de 0.556 a 0.323. La exactitud global se mantuvo relativamente estable, aunque se observó una ligera disminución en el recall, lo que muestra que la técnica logra equidad a costa de sacrificar parcialmente la capacidad de identificación de los casos positivos.</p>
<p>La segunda estrategia consistió en el ajuste de umbrales diferenciados por grupo con el objetivo de alcanzar la paridad demográfica. Se estableció un umbral de 0.60 para hombres y de 0.16 para mujeres. Esta decisión redujo la disparidad de género casi a cero, logrando un dp_diff de apenas 0.003. Sin embargo, la exactitud global del modelo descendió de 0.854 a 0.831, lo que pone de relieve la tensión entre equidad y desempeño. Además, al aplicar la misma estrategia en raza, la mejora fue más modesta.</p>
<p>La tercera estrategia aplicada fue <strong>Reweighing</strong>, que asigna pesos a las observaciones de acuerdo con la distribución conjunta de atributos sensibles e ingresos. En este caso, la disparidad racial se redujo de un eo_diff de 0.556 a 0.333, manteniendo estables las métricas globales de desempeño, con una exactitud cercana al 85%. Esta técnica resultó especialmente útil porque logra un equilibrio razonable sin afectar de manera significativa la calidad predictiva del modelo.</p>
<p><strong>Tabla 4. Disparidades por sexo</strong></p>
<table>
<thead>
<tr>
<th>Modelo</th>
<th>dp_diff</th>
<th>dp_ratio</th>
<th>eo_diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>0.169</td>
<td>0.327</td>
<td>0.068</td>
</tr>
<tr>
<td>Eq. Odds</td>
<td>0.104</td>
<td>0.506</td>
<td>0.031</td>
</tr>
<tr>
<td>Thresholding</td>
<td>0.003</td>
<td>0.985</td>
<td>0.319</td>
</tr>
<tr>
<td>Reweighing</td>
<td>0.118</td>
<td>0.428</td>
<td>0.333</td>
</tr>
</tbody>
</table>
<p><strong>Tabla 5. Disparidades por raza</strong></p>
<table>
<thead>
<tr>
<th>Modelo</th>
<th>dp_diff</th>
<th>dp_ratio</th>
<th>eo_diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>0.108</td>
<td>0.478</td>
<td>0.556</td>
</tr>
<tr>
<td>Eq. Odds</td>
<td>0.110</td>
<td>0.407</td>
<td>0.323</td>
</tr>
<tr>
<td>Thresholding</td>
<td>0.012</td>
<td>0.941</td>
<td>0.455</td>
</tr>
<tr>
<td>Reweighing</td>
<td>0.118</td>
<td>0.428</td>
<td>0.333</td>
</tr>
</tbody>
</table>
<h2 id="5-comparacion-de-resultados">5. Comparación de Resultados</h2>
<p>La comparación de las distintas estrategias muestra que no existe una solución única ni perfecta. En el caso del género, el ajuste de umbrales fue la técnica más efectiva para eliminar la disparidad en paridad demográfica, aunque sacrificó exactitud. Equalized Odds ofreció un balance más moderado, reduciendo de manera significativa las desigualdades sin afectar tanto las métricas globales. Reweighing, por su parte, mostró un desempeño intermedio, con mejoras aceptables y estabilidad en el rendimiento general.</p>
<p><strong>Tabla 6. Comparación sexo: baseline vs Equalized Odds</strong></p>
<table>
<thead>
<tr>
<th>Grupo</th>
<th>Accuracy Before</th>
<th>Accuracy After</th>
<th>Recall Before</th>
<th>Recall After</th>
<th>F1 Before</th>
<th>F1 After</th>
<th>Selection Rate Before</th>
<th>Selection Rate After</th>
</tr>
</thead>
<tbody>
<tr>
<td>Female</td>
<td>0.929</td>
<td>0.908</td>
<td>0.551</td>
<td>0.568</td>
<td>0.629</td>
<td>0.576</td>
<td>0.082</td>
<td>0.107</td>
</tr>
<tr>
<td>Male</td>
<td>0.818</td>
<td>0.812</td>
<td>0.615</td>
<td>0.537</td>
<td>0.673</td>
<td>0.636</td>
<td>0.252</td>
<td>0.211</td>
</tr>
</tbody>
</table>
<p>En el caso de la raza, Equalized Odds y Reweighing demostraron ser más efectivos que el ajuste de umbrales. Ambas técnicas redujeron el eo_diff de más de 0.55 a valores entre 0.32 y 0.33, lo que representa una mejora sustancial, aunque las disparidades no se eliminan por completo. El ajuste de umbrales, en cambio, tuvo un impacto limitado y no logró equiparar de forma adecuada la probabilidad de clasificación entre grupos raciales.</p>
<p><strong>Tabla 7. Comparación raza: baseline vs Equalized Odds</strong></p>
<table>
<thead>
<tr>
<th>Grupo</th>
<th>Accuracy Before</th>
<th>Accuracy After</th>
<th>Recall Before</th>
<th>Recall After</th>
<th>F1 Before</th>
<th>F1 After</th>
<th>Selection Rate Before</th>
<th>Selection Rate After</th>
</tr>
</thead>
<tbody>
<tr>
<td>Amer-Indian-Eskimo</td>
<td>0.873</td>
<td>0.861</td>
<td>0.444</td>
<td>0.222</td>
<td>0.444</td>
<td>0.267</td>
<td>0.114</td>
<td>0.076</td>
</tr>
<tr>
<td>Asian-Pac-Islander</td>
<td>0.825</td>
<td>0.829</td>
<td>0.561</td>
<td>0.530</td>
<td>0.622</td>
<td>0.614</td>
<td>0.206</td>
<td>0.187</td>
</tr>
<tr>
<td>Black</td>
<td>0.914</td>
<td>0.903</td>
<td>0.557</td>
<td>0.545</td>
<td>0.612</td>
<td>0.578</td>
<td>0.099</td>
<td>0.108</td>
</tr>
<tr>
<td>Other</td>
<td>0.937</td>
<td>0.889</td>
<td>1.000</td>
<td>0.333</td>
<td>0.600</td>
<td>0.222</td>
<td>0.111</td>
<td>0.095</td>
</tr>
<tr>
<td>White</td>
<td>0.848</td>
<td>0.838</td>
<td>0.609</td>
<td>0.544</td>
<td>0.672</td>
<td>0.631</td>
<td>0.208</td>
<td>0.185</td>
</tr>
</tbody>
</table>
<h2 id="6-conclusiones-y-reflexiones-eticas">6. Conclusiones y Reflexiones Éticas</h2>
<p>El análisis realizado confirma que los sesgos en los datos constituyen un riesgo real para los sistemas de IA. El dataset Adult Census Income refleja inequidades estructurales: los hombres y las personas blancas aparecen sobrerrepresentados en los niveles de ingreso alto. El modelo baseline, sin mitigación, hereda estas desigualdades y las refleja en sus predicciones, generando resultados discriminatorios para las mujeres y para las minorías raciales.</p>
<p>Las técnicas de mitigación aplicadas muestran que es posible reducir estas disparidades, aunque siempre existen compromisos entre equidad y precisión. Equalized Odds demostró ser una opción sólida para mejorar la equidad sin un costo excesivo en rendimiento. El ajuste de umbrales mostró que puede eliminar casi por completo la disparidad en género, pero no ofrece la misma efectividad en raza y sacrifica exactitud global. Reweighing se presentó como una estrategia intermedia que mantiene el desempeño general del modelo y reduce de manera importante las disparidades raciales.</p>
<p>Más allá de los resultados técnicos, este ejercicio invita a reflexionar sobre los riesgos de aplicar estos modelos en contextos reales. Si un sistema de este tipo se utilizara para procesos de contratación laboral, aprobación de créditos o asignación de beneficios, podría reforzar desigualdades históricas y generar un impacto negativo sobre grupos ya desfavorecidos. Esto subraya la importancia de no limitar la solución al plano algorítmico, sino acompañarla con medidas organizacionales y políticas públicas. Es necesario mejorar la recolección de datos para garantizar una representación más balanceada, realizar auditorías periódicas de equidad y asegurar que la decisión final siempre contemple la supervisión humana y principios éticos de no discriminación.</p>
<p>En conclusión, la aplicación de técnicas de mitigación permite avanzar hacia sistemas de IA más justos y responsables. Sin embargo, ninguna técnica elimina completamente los sesgos, lo que obliga a reconocer que la equidad es un objetivo que requiere tanto intervenciones técnicas como compromisos institucionales y sociales.</p>